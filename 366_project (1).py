# -*- coding: utf-8 -*-
"""366_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v6FgSY5zIAyt5l6M1ViXV62C9mmhe8J1
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import missingno as msno
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

df=pd.read_csv('/content/drive/MyDrive/cse_project/autism_screening.csv')
df

df.head()

df.tail()

"""# Data Preprocessing"""

# Replace
df = df.replace('?', np.nan)

# Rename
df.rename(columns={'jundice': 'jaundice'}, inplace=True)

# Rename
df.rename(columns={'austim': 'autism'}, inplace=True)

# Check missing values
df.isnull().sum()

#check duplicate values
duplicates = df.duplicated()
print("Number of duplicate rows:", duplicates.sum())

#to see the duplicate rows themselves
print(df[duplicates])

# Missing values visualisation
msno.bar(df)

# Recheck Head
df.head()

# Handeling missing values
median_value = df['age'].median()
df['age'] = df['age'].fillna(median_value)

# Handeling missing values
mode_value = df['relation'].mode()[0]
df['relation'] = df['relation'].fillna(mode_value)

# Handeling missing values
mode_value = df['ethnicity'].mode()[0]
df['ethnicity'] = df['ethnicity'].fillna(mode_value)

# Rechecking missing values
df.isnull().sum()

"""# Visualization"""

plt.figure(figsize=(12, 8))
sns.countplot(data=df, x='ethnicity', hue='Class/ASD', palette='viridis')
plt.title('ASD Class:Across Different Ethnicities', fontsize=16)
plt.xlabel('Ethnicity', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks(rotation=45)
plt.show()

custom_bins = [10, 20, 30, 40, 50, 60, 70, 80, 100]
plt.figure(figsize=(12, 6))
plt.hist(df['age'], bins=custom_bins, edgecolor='black', color='skyblue')
plt.title('Age Distribution of Participants', fontsize=16)
plt.xlabel('Age', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.show()

"""# Data Cleaning & Preparation"""

df['gender'].value_counts()

# Define the mapping dictionary
gender_mapping = {
    'm': 0,
    'f': 1,
}

# Apply the mapping
df['gender'] = df['gender'].map(gender_mapping)

df['ethnicity'].value_counts()

# Checking issue with ethnicity
print(df['ethnicity'].unique())

ethnicity_mapping = {
    'White-European': 0,
    'Asian': 1,
    'Middle Eastern ': 2,
    'Black': 3,
    'South Asian': 4,
    'Others': 5,
    'Latino': 6,
    'Hispanic': 7,
    'Pasifika': 8,
    'Turkish': 9,
    'others': 10,
}

# Apply the mapping
df['ethnicity'] = df['ethnicity'].map(ethnicity_mapping)

df['jaundice'].value_counts()

# Define the mapping dictionary
jaundice_mapping = {
    'no': 0,
    'yes': 1,
}

# Apply the mapping
df['jaundice'] = df['jaundice'].map(jaundice_mapping)

df['autism'].value_counts()

# Define the mapping dictionary
autism_mapping = {
    'no': 0,
    'yes': 1,
}

# Apply the mapping
df['autism'] = df['autism'].map(autism_mapping)

df['relation'].value_counts()

# Define the mapping dictionary
relation_mapping = {
    'Self': 0,
    'Parent': 1,
    'Relative': 2,
    'Others': 3,
    'Health care professional': 4,
}

# Apply the mapping
df['relation'] = df['relation'].map(relation_mapping)

df['Class/ASD'].value_counts()

# Define the mapping dictionary
asd_mapping = {
    'NO': 0,
    'YES': 1,
}

# Apply the mapping
df['Class/ASD'] = df['Class/ASD'].map(asd_mapping)

df['Class/ASD'].value_counts()

df= df.drop(columns=['contry_of_res', 'age_desc', 'used_app_before'], axis=1)
df

"""# Visualization"""

# Correlation Matrix
df.corr()
plt.figure(figsize=(15,8))
sns.heatmap(df.corr(), annot=True)

"""# Model Training"""

# Columns in df has been dropped previousy
X=df.drop(columns=['Class/ASD'], axis=1)
y=df['Class/ASD']

# Test & Train
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)

"""# LOGISTIC REGRESSION"""

lr = LogisticRegression()

lr.fit(X_train, y_train)
y_pred=lr.predict(X_test)

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Precision
precision = precision_score(y_test, y_pred)
print("Precision:", precision)

# Recall (sensitivity)
recall = recall_score(y_test, y_pred)
print("Recall (Sensitivity):", recall)

# F1-score
f1 = f1_score(y_test, y_pred)
print("F1-Score:", f1)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.grid(False)
plt.title("Confusion Matrix")
plt.show()

"""# SUPPORT VECTOR MACHINE"""

svm = SVC(kernel='linear', random_state=0)
svm.fit(X_train, y_train)

y_pred = svm.predict(X_test)

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Precision
precision = precision_score(y_test, y_pred)
print("Precision:", precision)

# Recall (sensitivity)
recall = recall_score(y_test, y_pred)
print("Recall (Sensitivity):", recall)

# F1-score
f1 = f1_score(y_test, y_pred)
print("F1-Score:", f1)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.grid(False)
plt.title("Confusion Matrix")
plt.show()

"""# DECISION TREE"""

dc = DecisionTreeClassifier(random_state=51)
dc.fit(X_train, y_train)
y_pred = dc.predict(X_test)

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Precision
precision = precision_score(y_test, y_pred)
print("Precision:", precision)

# Recall (sensitivity)
recall = recall_score(y_test, y_pred)
print("Recall (Sensitivity):", recall)

# F1-score
f1 = f1_score(y_test, y_pred)
print("F1-Score:", f1)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.grid(False)
plt.title("Confusion Matrix")
plt.show()

"""# ARTIFICIAL NEURAL NETWORK (ANN)"""

# Building ANN
ann = Sequential([
    Dense(16, input_shape=(X_train.shape[1],), activation='relu'), # 1st hidden layer, shape=input=17
    Dense(8, activation='relu'), # 2nd Hidden layer
    Dense(1, activation='sigmoid') # Output layer for binary
])

# Compile model
ann.compile(
    optimizer='adam',
    loss='binary_crossentropy',  # Binary classification loss
    metrics=['accuracy']
)

# Train model
ann.fit(X_train, y_train, epochs=30, batch_size=32, verbose=1)
# Evaluate model
loss, accuracy = ann.evaluate(X_test, y_test)
print("Test accuracy:", accuracy)

ann.summary()

# Predict probabilities
y_pred_prob = ann.predict(X_test)

# Convert probabilities to binary
y_pred = (y_pred_prob > 0.5).astype("int32")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.grid(False)
plt.title("Confusion Matrix (")
plt.show()

model_names = ['Logistic Regression', 'SVM', 'Decision Tree', 'ANN']
accuracies = [1.0, 1.0, 1.0, 0.93]
plt.bar(model_names, accuracies)
for i in range(len(accuracies)):
    plt.text(i, accuracies[i], round(accuracies[i], 2), ha='center')

plt.ylabel('Accuracy')
plt.title('Model Accuracy Comparison')
plt.show()